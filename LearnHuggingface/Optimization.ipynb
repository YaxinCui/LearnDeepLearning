{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    ".optimization\n",
    "- 权重优化器，可用于微调模型\n",
    "- 调度对象，继承于 _LRSchedule\n",
    "- 一个梯度积累类，累计多个批次的梯度\n",
    "\n",
    "transformers.AdamW(params: Iterable[torch.nn.paramerter.Parameter], lr: float=0.001, betas:Tuple[float, float]=0.9, 0.999, eps:float=1e-06; weigth_decay:float=0.0, correct_bias:bool=True)\n",
    "    parames()\n",
    "    lr\n",
    "    betas: Adam的beta参数\n",
    "    eps:稳定性\n",
    "    weight_decay: 权重衰减\n",
    "    correct_bias：纠正Adam中的偏差。\n",
    "\n",
    "step(closure: Callable=None):\n",
    "    参数\n",
    "    closure(, optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdamWeightDecay(TensorFlow)\n",
    "class transformers.AdamWeightDecay(learning_rate:Union[float, tensorflow.python.keras.optimizer_v2.learning_rate_schedule.LearningRateSchedule]=0.001, beata_1:float=0.9, beta_2:float=0.999, epsilon:float=1e-07, amsgrad:bool=False, weight_decay_rate:float=0.0, include_in_weight_decay:Optional[List[str]]=None, ...)\n",
    "    Adam支持L2权重衰减，只是将权重的平方添加到损失函数，不是使用L2正则化/权重衰减等。\n",
    "    相反，外面希望不要以m/v参数交互的方式衰减权重。这相当于使用普通（非动量）SGD将权重的平方添加到损耗中\n",
    "    learning_rate 1e-3\n",
    "    beta_1 0.9\n",
    "    beta_2 0.999\n",
    "    epsilon 1e-7\n",
    "    amsgrad False，是否应用此算法的AMSGrad变数与否\n",
    "    weight_decay_rate 0 要应用的权重衰减\n",
    "    include_in_weight_decay List 要应用权重衰减的参数名称。\n",
    "    exclude_from_weight_decay List：要排除对权重衰减的参数名称。\n",
    "    名称：默认为\"AdamWeigthDecay\" = 应用渐变时创建的操作的可选名称。\n",
    "    kwargs = [clipnorm, clipvalue, lr, decay]\n",
    "    \n",
    "transformers.create_optimizer(init_lr:float, num_train_steps:int, num_warmup_steps:int, min_lr_ratio:float=0.0, adam_epsilon:float=1e-8, weight_decay_rate:float=0.0, include_in_weight_decay:Optional[List[str]])\n",
    "\n",
    "warm_up：一种权重变换策略。\n",
    "\n",
    "\n",
    "init_lr 预热阶段学习速率\n",
    "num_train_step 培训步骤的总数\n",
    "num_warmup_steps 预热步骤数\n",
    "min_lr_ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schedules\n",
    "Learning Rate Schedules(Pytorch)\n",
    "transformers.get_constant_schedule()\n",
    "optimizer\n",
    "last_epoch\n",
    "\n",
    "torch.optim.lr_scheduler.LambdaLR\n",
    "\n",
    "transformers.get_constant_schedule_with_warmup(optimizer:torch.optim.optimizer.Optimizer, num_warmup_steps:int, last_epoch:int=-1)\n",
    "\n",
    "optimizer\n",
    "num_warmup_steps\n",
    "last_epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
