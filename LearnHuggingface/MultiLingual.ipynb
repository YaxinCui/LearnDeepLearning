{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多语种模型\n",
    "# 提供的主要模型是单语模型，也提供一些多语言模型，其机制与单语模型不同。\n",
    "# 目前支持多种语言的两种模型是 BERT XLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 1.45M/1.45M [00:12<00:00, 115kB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 1.02k/1.02k [00:00<00:00, 178kB/s]\n",
      "Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 830M/830M [03:51<00:00, 3.59MB/s]\n"
     ]
    }
   ],
   "source": [
    "# XLM，共有10个不同的检查点，其中只有一个是单语检查点，其余9个模型检查点可以分为两类：使用语言嵌入的检查点和不使用语言嵌入的检查点。\n",
    "\n",
    "# xlm-mlm-ende-1024\n",
    "# xlm-mlm-enfr-2024\n",
    "# xlm-mlm-enro-1024\n",
    "# xlm-mlm-xn-xnli15-1024\n",
    "# xlm-mlm-tlm-xnli15-1024\n",
    "# xlm-clm-enfr-1024\n",
    "# xlm-clm-ende-1024\n",
    "\n",
    "import torch\n",
    "from transformers import XLMTokenizer, XLMWithLMHeadModel\n",
    "\n",
    "tokenizer = XLMTokenizer.from_pretrained(\"xlm-clm-enfr-1024\")\n",
    "model = XLMWithLMHeadModel.from_pretrained(\"xlm-clm-enfr-1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en': 0, 'fr': 1}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.lang2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,  4018,  5545, 51104,    32,   308,    18,     1]])\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor([tokenizer.encode(\"Wikipedia was used to\")]) # batch size of 1\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_id = tokenizer.lang2id['en'] # 0\n",
    "langs = torch.tensor([language_id] * input_ids.shape[1]) # torch.tensor([0,0,0...0])\n",
    "\n",
    "langs = langs.view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(input_ids, langs=langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert, bert-case-multilingual-uncased，下一个句子预测，102种语言\n",
    "# bert, bert-case-multilingual-cased，下一个句子预测，104种语言\n",
    "\n",
    "# XLM-RoBERTa，以100种语言，对2.5TB的干净公共爬取数据进行训练。\n",
    "# 与以前发布的多语种模型（mBERT XLM）在下游标记（如分类、序列标记、问题解答）\n",
    "\n",
    "# xlm-roberta-base\n",
    "# xlm-roberta-large"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('cui': conda)",
   "language": "python",
   "name": "python361064bitcuiconda024f7caa9f8b4d7b8d9f3be3d74987fc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
