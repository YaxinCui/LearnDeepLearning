{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformers是一个\n",
    "1. NLP研究者使用/学习/拓展 transformers模型的工具。\n",
    "2. 使用微调。\n",
    "3. 工程师\n",
    "\n",
    "两个强有力的目标\n",
    "1. 尽可能简单快捷地使用。\n",
    "    - 三个标准类，configuration、models、tokenizer\n",
    "    - 可以以简单、统一的方法初始化，从HuggingFace Hub提供的下载点、缓存和加载。(from_pretrained函数)\n",
    "    - 还提供了两个API，pipeline()，Trainer()/TFTrainer()\n",
    "    - 此库不是神经网络构建基本模块化工具箱，如果要拓展/构建库，只需使用常规python/pytorch/tensorflow/keras模块。\n",
    "2. 提供最先进的模型，其性能尽可能接近原始模型\n",
    "- 至少为每个结构提供一个示例，示例再现了该结构的官方作者结果。\n",
    "- 代码通常尽可能接近原始代码库。\n",
    "\n",
    "其他目标\n",
    "1. 尽可能一直公开模型的内部\n",
    "    - 使用单个API访问内部隐藏状态、attention权重\n",
    "    - tokenizer和基本模型的API是标准化的，可以轻松在模型之间切换\n",
    "2. 纳入一系列工具，可以微调/调查这些模型。\n",
    "    - 有简单一致的方法向模型添加新token到词向量中，用于微调。\n",
    "    - 可以简单地mask和prune transformer头。\n",
    "3. 简单地从tensorflow和pytorch之间进行转换。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意概念\n",
    "Model类，如BertModel类，提供了基于不同框架的不同版本。\n",
    "Configuration类，如BertConfig类，存储生成模型所需参数，不总是需要自己实例化这些。如果使用的是预训练模型，创建模型时，会自动实例化配置\n",
    "Tokenizer类，如Bert Tokenizer，存储每个模型的词汇表，提供编码/解码\n",
    "\n",
    "from_pretrained()\n",
    "save_pretrained()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
