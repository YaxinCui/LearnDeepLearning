{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! wget http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "# tar -zxvf cifar-10-python.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch import optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from torchvision import models, transforms\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # plt 用于显示图片\n",
    "import time\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(np_image, title = None):\n",
    "    # np_image是numpy矩阵类型，形状为(3, 32, 32)\n",
    "    np_image = np_image.transpose((1, 2, 0))\n",
    "    plt.imshow(np_image)\n",
    "    if title is not None:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/cifar-10-batches-py/test_batch', 'rb') as fo:\n",
    "    testData = pickle.load(fo, encoding='bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "trainData = {b'batch_label':[], b'labels':[], b'data': np.zeros(shape=(0, 3072), dtype='uint8'), b'filenames':[]}\n",
    "\n",
    "for i in range(1, 6):\n",
    "    with open('data/cifar-10-batches-py/data_batch_'+str(i), 'rb') as fo:\n",
    "        data_batch = pickle.load(fo, encoding='bytes')\n",
    "        trainData[b'labels'] += data_batch[b'labels']\n",
    "        trainData[b'data'] = np.vstack((trainData[b'data'], data_batch[b'data']))\n",
    "        trainData[b'filenames'] += data_batch[b'filenames']\n",
    "print(trainData.keys())\n",
    "print(len(trainData[b'data']))\n",
    "\n",
    "# 所有类型\n",
    "index2label = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "# 1-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampleIndex = 40\n",
    "sampleData = testData[b'data'][sampleIndex]\n",
    "sampleLabel = testData[b'labels'][sampleIndex]\n",
    "show(sampleData.reshape((3, 32, 32)), title=index2label[sampleLabel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10Dataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        # type(data): numpy.arrays shape [batch * 3 * 32 * 32]\n",
    "        # type(labels): list int\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        \n",
    "        assert len(self.data) == len(self.labels), \"CIFAR10 DATASET data长度与labels长度不一样\"\n",
    "        self.length = len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.data[index].reshape((3, 32, 32)), self.labels[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'transformer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d63e3f705abd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrainDataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCIFAR10Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mb'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mb'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainDataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtestDataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCIFAR10Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mb'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mb'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtestDataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'transformer'"
     ]
    }
   ],
   "source": [
    "trainDataset = CIFAR10Dataset(trainData[b'data'], trainData[b'labels'])\n",
    "trainDataloader = DataLoader(trainDataset, batch_size=16, shuffle=False, transformer=transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)))\n",
    "\n",
    "testDataset = CIFAR10Dataset(testData[b'data'], testData[b'labels'])\n",
    "testDataloader = DataLoader(testDataset, batch_size=16, shuffle=False, transformers=transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10Module(nn.Module):\n",
    "    \n",
    "    def __init__(self, labelSize=10, bias=False):\n",
    "        # 3 32 32 -->  16, 32, 32 --> 16, 16, 16 --> 16, 8, 8 --> 4, 8, 8 --> 128\n",
    "        super(CIFAR10Module, self).__init__()\n",
    "        \n",
    "        self.Conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=(1, 1), dilation=1, groups=1, bias=True, padding_mode=\"zeros\")\n",
    "        self.Relu = nn.ReLU()\n",
    "        self.Maxpool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, return_indices=False, ceil_mode=False)\n",
    "        self.Avgpool = nn.AvgPool2d(2)\n",
    "        self.Conv2 = nn.Conv2d(16, 4, 3, padding=(1, 1))\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        self.Tanh = nn.Tanh()\n",
    "        \n",
    "        self.linear = nn.Linear(4 * 8 * 8, labelSize)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input = input.view((-1, 3, 32, 32))\n",
    "        output = self.Conv1(input) # (-1, 3, 32, 32) --> (-1, 16, 32, 32)\n",
    "        \n",
    "        output = self.Relu(output)\n",
    "        output = self.Maxpool(output) # 16 x 32 x 32 --> 16 x 16 x 16\n",
    "        output = self.Sigmoid(output)\n",
    "        output = self.Avgpool(output) # 16 x 16 x 16 --> 16 x 8 x 8\n",
    "        output = self.Tanh(output)\n",
    "        output = self.Conv2(output) # 16 x 8 x 8 --> 4 x 8 x 8\n",
    "        output = output.view(-1, 4 * 8 * 8)\n",
    "        output = self.linear(output)\n",
    "        output = self.softmax(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "cifar10Model = CIFAR10Module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cifar10Model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-09b2e4e1c875>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcifar10Model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.999\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-08\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cifar10Model' is not defined"
     ]
    }
   ],
   "source": [
    "# 自己的模型\n",
    "optimizerMe = optim.Adam(params=cifar10Model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "exp_lr_schedulerMe = lr_scheduler.StepLR(optimizerMe, step_size=10, gamma=0.1)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (inputs, labels) in enumerate(trainDataloader):\n",
    "        cifar10Model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = cifar10Model(inputs.float())\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {'train':trainDataloader, 'val':testDataloader}\n",
    "def train_model(model, criterion, optimizer, scheduler, dataloaders, skip = 100, num_epochs=25):\n",
    "    # \n",
    "    import time\n",
    "    import copy\n",
    "    \n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # 每个epoch都有一个训练和验证的阶段\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            # 迭代数据\n",
    "            for i, (inputs, labels) in enumerate(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # 零参数梯度\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # 前向\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs.float())\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    # 后向仅在训练阶段进行优化\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                # 统计\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                if i % skip == 0:\n",
    "                    print('{} Loss: {:.4f} Acc: {:.4f}={}/{}'.format(phase, loss.item(), torch.sum(preds == labels.data).double()/inputs.size(0), torch.sum(preds == labels.data), inputs.size(0)))\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase])\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase])\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            # 深度复制\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        print()\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training compete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    \n",
    "    # 加载最佳模型权重\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 2.1551 Acc: 0.0625=1/16\n",
      "train Loss: 2.5954 Acc: 0.0625=1/16\n",
      "train Loss: 2.6874 Acc: 0.3125=5/16\n",
      "train Loss: 2.8377 Acc: 0.0000=0/16\n",
      "train Loss: 2.6365 Acc: 0.1250=2/16\n",
      "train Loss: 2.5230 Acc: 0.0625=1/16\n",
      "train Loss: 2.3941 Acc: 0.1250=2/16\n",
      "train Loss: 2.5498 Acc: 0.0625=1/16\n",
      "train Loss: 2.6075 Acc: 0.1250=2/16\n",
      "train Loss: 2.5617 Acc: 0.1250=2/16\n",
      "train Loss: 2.5052 Acc: 0.0625=1/16\n",
      "train Loss: 2.5948 Acc: 0.1250=2/16\n",
      "train Loss: 2.5395 Acc: 0.1250=2/16\n",
      "train Loss: 2.9053 Acc: 0.0625=1/16\n",
      "train Loss: 2.3166 Acc: 0.1875=3/16\n",
      "train Loss: 2.6188 Acc: 0.0625=1/16\n",
      "train Loss: 2.4401 Acc: 0.0625=1/16\n",
      "train Loss: 2.6640 Acc: 0.0000=0/16\n",
      "train Loss: 2.5208 Acc: 0.0000=0/16\n",
      "train Loss: 2.6140 Acc: 0.0625=1/16\n",
      "train Loss: 2.5304 Acc: 0.1250=2/16\n",
      "train Loss: 2.4536 Acc: 0.1250=2/16\n",
      "train Loss: 2.6741 Acc: 0.1250=2/16\n",
      "train Loss: 2.3686 Acc: 0.1250=2/16\n",
      "train Loss: 2.7675 Acc: 0.1250=2/16\n",
      "train Loss: 2.8303 Acc: 0.0625=1/16\n",
      "train Loss: 2.4347 Acc: 0.1250=2/16\n",
      "train Loss: 2.4208 Acc: 0.0625=1/16\n",
      "train Loss: 2.6672 Acc: 0.1250=2/16\n",
      "train Loss: 2.2534 Acc: 0.2500=4/16\n",
      "train Loss: 2.8789 Acc: 0.0000=0/16\n",
      "train Loss: 2.6108 Acc: 0.1250=2/16\n",
      "train Loss: 2.3563 Acc: 0.0625=1/16\n",
      "train Loss: 2.7484 Acc: 0.0000=0/16\n",
      "train Loss: 3.0692 Acc: 0.0000=0/16\n",
      "train Loss: 2.3538 Acc: 0.1250=2/16\n",
      "train Loss: 2.7951 Acc: 0.1875=3/16\n",
      "train Loss: 2.5048 Acc: 0.0625=1/16\n",
      "train Loss: 2.5482 Acc: 0.3125=5/16\n",
      "train Loss: 2.6017 Acc: 0.1875=3/16\n",
      "train Loss: 2.4906 Acc: 0.0625=1/16\n",
      "train Loss: 2.6715 Acc: 0.1250=2/16\n",
      "train Loss: 2.4319 Acc: 0.0000=0/16\n",
      "train Loss: 2.6856 Acc: 0.0000=0/16\n",
      "train Loss: 2.5751 Acc: 0.1250=2/16\n",
      "train Loss: 2.7093 Acc: 0.0000=0/16\n",
      "train Loss: 2.7749 Acc: 0.1250=2/16\n",
      "train Loss: 2.4133 Acc: 0.1875=3/16\n",
      "train Loss: 2.5876 Acc: 0.0000=0/16\n",
      "train Loss: 2.5048 Acc: 0.0000=0/16\n",
      "train Loss: 2.7029 Acc: 0.2500=4/16\n",
      "train Loss: 2.4267 Acc: 0.1250=2/16\n",
      "train Loss: 2.4627 Acc: 0.0625=1/16\n",
      "train Loss: 2.8644 Acc: 0.0000=0/16\n",
      "train Loss: 2.3404 Acc: 0.0625=1/16\n",
      "train Loss: 2.4732 Acc: 0.0625=1/16\n",
      "train Loss: 2.6427 Acc: 0.0000=0/16\n",
      "train Loss: 2.4688 Acc: 0.0000=0/16\n",
      "train Loss: 2.3470 Acc: 0.2500=4/16\n",
      "train Loss: 2.4922 Acc: 0.0625=1/16\n",
      "train Loss: 2.3791 Acc: 0.0625=1/16\n",
      "train Loss: 2.5227 Acc: 0.0625=1/16\n",
      "train Loss: 2.8269 Acc: 0.0625=1/16\n",
      "train Loss: 2.3745 Acc: 0.0625=1/16\n",
      "train Loss: 2.7512 Acc: 0.0625=1/16\n",
      "train Loss: 2.4131 Acc: 0.1250=2/16\n",
      "train Loss: 2.6582 Acc: 0.0625=1/16\n",
      "train Loss: 2.5386 Acc: 0.1875=3/16\n",
      "train Loss: 2.4772 Acc: 0.0000=0/16\n",
      "train Loss: 2.3185 Acc: 0.0000=0/16\n",
      "train Loss: 2.5557 Acc: 0.0625=1/16\n",
      "train Loss: 2.3443 Acc: 0.1250=2/16\n",
      "train Loss: 2.4382 Acc: 0.2500=4/16\n",
      "train Loss: 2.5541 Acc: 0.1250=2/16\n",
      "train Loss: 2.3115 Acc: 0.0625=1/16\n",
      "train Loss: 2.3910 Acc: 0.1250=2/16\n",
      "train Loss: 2.5401 Acc: 0.0625=1/16\n",
      "train Loss: 2.3948 Acc: 0.1250=2/16\n",
      "train Loss: 2.3320 Acc: 0.1250=2/16\n",
      "train Loss: 2.4011 Acc: 0.1250=2/16\n",
      "train Loss: 2.3596 Acc: 0.1875=3/16\n",
      "train Loss: 2.4076 Acc: 0.1250=2/16\n",
      "train Loss: 2.4659 Acc: 0.0625=1/16\n",
      "train Loss: 2.5120 Acc: 0.1250=2/16\n",
      "train Loss: 2.4954 Acc: 0.0625=1/16\n",
      "train Loss: 2.7087 Acc: 0.1875=3/16\n",
      "train Loss: 2.2701 Acc: 0.2500=4/16\n",
      "train Loss: 2.6509 Acc: 0.2500=4/16\n",
      "train Loss: 2.7061 Acc: 0.1250=2/16\n",
      "train Loss: 2.4743 Acc: 0.0625=1/16\n",
      "train Loss: 2.4038 Acc: 0.1250=2/16\n",
      "train Loss: 2.3911 Acc: 0.0625=1/16\n",
      "train Loss: 2.3213 Acc: 0.1250=2/16\n",
      "train Loss: 2.6395 Acc: 0.0625=1/16\n",
      "train Loss: 2.6015 Acc: 0.0000=0/16\n",
      "train Loss: 2.3838 Acc: 0.0625=1/16\n",
      "train Loss: 2.5273 Acc: 0.0625=1/16\n",
      "train Loss: 2.3512 Acc: 0.1875=3/16\n",
      "train Loss: 2.4427 Acc: 0.1875=3/16\n",
      "train Loss: 2.6521 Acc: 0.1250=2/16\n",
      "train Loss: 2.4269 Acc: 0.1875=3/16\n",
      "train Loss: 2.4951 Acc: 0.0625=1/16\n",
      "train Loss: 2.6055 Acc: 0.0625=1/16\n",
      "train Loss: 2.5293 Acc: 0.1250=2/16\n",
      "train Loss: 2.6691 Acc: 0.0625=1/16\n",
      "train Loss: 2.4604 Acc: 0.1250=2/16\n",
      "train Loss: 2.5267 Acc: 0.0000=0/16\n",
      "train Loss: 2.5385 Acc: 0.1250=2/16\n",
      "train Loss: 2.5348 Acc: 0.0625=1/16\n",
      "train Loss: 2.5332 Acc: 0.1250=2/16\n",
      "train Loss: 2.2338 Acc: 0.1875=3/16\n",
      "train Loss: 2.4127 Acc: 0.1250=2/16\n",
      "train Loss: 2.6152 Acc: 0.0625=1/16\n",
      "train Loss: 2.4986 Acc: 0.1250=2/16\n",
      "train Loss: 2.6680 Acc: 0.1875=3/16\n",
      "train Loss: 2.3872 Acc: 0.1250=2/16\n",
      "train Loss: 2.4568 Acc: 0.0000=0/16\n",
      "train Loss: 2.6738 Acc: 0.0625=1/16\n",
      "train Loss: 2.4084 Acc: 0.1875=3/16\n",
      "train Loss: 2.4303 Acc: 0.1875=3/16\n",
      "train Loss: 2.2986 Acc: 0.1875=3/16\n",
      "train Loss: 2.6914 Acc: 0.0625=1/16\n",
      "train Loss: 2.3268 Acc: 0.3125=5/16\n",
      "train Loss: 2.5007 Acc: 0.0000=0/16\n",
      "train Loss: 2.3895 Acc: 0.1875=3/16\n",
      "train Loss: 2.3979 Acc: 0.0000=0/16\n",
      "train Loss: 2.4462 Acc: 0.0625=1/16\n",
      "train Loss: 2.3945 Acc: 0.0625=1/16\n",
      "train Loss: 2.3342 Acc: 0.2500=4/16\n",
      "train Loss: 2.6281 Acc: 0.0000=0/16\n",
      "train Loss: 2.6653 Acc: 0.0625=1/16\n",
      "train Loss: 2.4561 Acc: 0.0625=1/16\n",
      "train Loss: 2.5095 Acc: 0.2500=4/16\n",
      "train Loss: 2.3528 Acc: 0.1875=3/16\n",
      "train Loss: 2.4755 Acc: 0.1875=3/16\n",
      "train Loss: 2.5735 Acc: 0.0625=1/16\n",
      "train Loss: 2.5903 Acc: 0.0625=1/16\n",
      "train Loss: 2.7246 Acc: 0.0000=0/16\n",
      "train Loss: 2.4370 Acc: 0.0625=1/16\n",
      "train Loss: 2.6604 Acc: 0.0000=0/16\n",
      "train Loss: 2.6329 Acc: 0.0000=0/16\n",
      "train Loss: 2.5356 Acc: 0.1250=2/16\n",
      "train Loss: 2.4822 Acc: 0.1250=2/16\n",
      "train Loss: 2.4932 Acc: 0.0625=1/16\n",
      "train Loss: 2.7396 Acc: 0.0625=1/16\n",
      "train Loss: 2.7830 Acc: 0.0625=1/16\n",
      "train Loss: 2.5306 Acc: 0.0625=1/16\n",
      "train Loss: 2.9664 Acc: 0.0625=1/16\n",
      "train Loss: 2.6927 Acc: 0.1250=2/16\n",
      "train Loss: 2.5750 Acc: 0.1875=3/16\n",
      "train Loss: 2.6930 Acc: 0.0625=1/16\n",
      "train Loss: 2.7225 Acc: 0.0625=1/16\n",
      "train Loss: 2.6470 Acc: 0.1250=2/16\n",
      "train Loss: 2.6113 Acc: 0.0625=1/16\n",
      "train Loss: 2.3905 Acc: 0.0625=1/16\n",
      "train Loss: 2.7572 Acc: 0.0000=0/16\n",
      "train Loss: 2.3965 Acc: 0.1250=2/16\n",
      "train Loss: 2.5287 Acc: 0.1875=3/16\n",
      "train Loss: 2.7123 Acc: 0.0000=0/16\n",
      "train Loss: 2.6669 Acc: 0.0000=0/16\n",
      "train Loss: 2.4987 Acc: 0.0625=1/16\n",
      "train Loss: 2.3990 Acc: 0.1875=3/16\n",
      "train Loss: 2.5218 Acc: 0.1875=3/16\n",
      "train Loss: 2.6710 Acc: 0.0000=0/16\n",
      "train Loss: 2.6112 Acc: 0.0000=0/16\n",
      "train Loss: 2.8490 Acc: 0.0000=0/16\n",
      "train Loss: 2.4063 Acc: 0.1250=2/16\n",
      "train Loss: 2.4046 Acc: 0.0625=1/16\n",
      "train Loss: 2.4694 Acc: 0.0625=1/16\n",
      "train Loss: 2.4897 Acc: 0.1250=2/16\n",
      "train Loss: 2.4740 Acc: 0.1250=2/16\n",
      "train Loss: 2.8979 Acc: 0.0000=0/16\n",
      "train Loss: 2.6056 Acc: 0.1875=3/16\n",
      "train Loss: 2.5319 Acc: 0.0000=0/16\n",
      "train Loss: 2.3257 Acc: 0.1250=2/16\n",
      "train Loss: 2.6912 Acc: 0.0625=1/16\n",
      "train Loss: 2.5193 Acc: 0.0000=0/16\n",
      "train Loss: 2.5709 Acc: 0.0625=1/16\n",
      "train Loss: 2.7012 Acc: 0.0000=0/16\n",
      "train Loss: 2.8056 Acc: 0.0000=0/16\n",
      "train Loss: 2.4908 Acc: 0.2500=4/16\n",
      "train Loss: 2.6694 Acc: 0.0000=0/16\n",
      "train Loss: 2.4639 Acc: 0.1250=2/16\n",
      "train Loss: 2.5317 Acc: 0.1250=2/16\n",
      "train Loss: 2.0850 Acc: 0.1875=3/16\n",
      "train Loss: 2.6390 Acc: 0.0625=1/16\n",
      "train Loss: 2.3994 Acc: 0.0625=1/16\n",
      "train Loss: 2.4555 Acc: 0.0625=1/16\n",
      "train Loss: 2.7048 Acc: 0.0625=1/16\n",
      "train Loss: 2.6112 Acc: 0.0625=1/16\n",
      "train Loss: 2.2520 Acc: 0.2500=4/16\n",
      "train Loss: 2.6321 Acc: 0.1250=2/16\n",
      "train Loss: 2.4596 Acc: 0.1875=3/16\n",
      "train Loss: 2.3427 Acc: 0.1250=2/16\n",
      "train Loss: 2.7326 Acc: 0.0625=1/16\n",
      "train Loss: 2.6017 Acc: 0.1250=2/16\n",
      "train Loss: 2.3730 Acc: 0.1875=3/16\n",
      "train Loss: 2.8762 Acc: 0.0000=0/16\n",
      "train Loss: 2.6526 Acc: 0.0625=1/16\n",
      "train Loss: 2.5569 Acc: 0.1875=3/16\n",
      "train Loss: 2.4401 Acc: 0.0625=1/16\n",
      "train Loss: 2.4017 Acc: 0.1250=2/16\n",
      "train Loss: 2.1838 Acc: 0.3125=5/16\n",
      "train Loss: 2.8656 Acc: 0.0000=0/16\n",
      "train Loss: 2.3046 Acc: 0.2500=4/16\n",
      "train Loss: 2.2001 Acc: 0.2500=4/16\n",
      "train Loss: 2.6634 Acc: 0.0625=1/16\n",
      "train Loss: 2.5670 Acc: 0.0625=1/16\n",
      "train Loss: 2.6318 Acc: 0.1250=2/16\n",
      "train Loss: 2.6981 Acc: 0.0625=1/16\n",
      "train Loss: 2.7823 Acc: 0.0625=1/16\n",
      "train Loss: 2.2969 Acc: 0.2500=4/16\n",
      "train Loss: 2.5858 Acc: 0.0625=1/16\n",
      "train Loss: 2.4743 Acc: 0.0000=0/16\n",
      "train Loss: 2.6583 Acc: 0.1875=3/16\n",
      "train Loss: 2.6642 Acc: 0.0000=0/16\n",
      "train Loss: 2.5642 Acc: 0.1250=2/16\n",
      "train Loss: 2.5238 Acc: 0.0625=1/16\n",
      "train Loss: 2.7321 Acc: 0.1875=3/16\n",
      "train Loss: 2.5181 Acc: 0.0625=1/16\n",
      "train Loss: 2.7362 Acc: 0.0625=1/16\n",
      "train Loss: 2.4580 Acc: 0.1875=3/16\n",
      "train Loss: 2.5941 Acc: 0.0625=1/16\n",
      "train Loss: 2.3700 Acc: 0.1875=3/16\n",
      "train Loss: 2.4434 Acc: 0.2500=4/16\n",
      "train Loss: 2.6466 Acc: 0.0000=0/16\n",
      "train Loss: 2.3734 Acc: 0.3125=5/16\n",
      "train Loss: 2.7371 Acc: 0.0000=0/16\n",
      "train Loss: 2.3279 Acc: 0.1250=2/16\n",
      "train Loss: 2.4883 Acc: 0.0625=1/16\n",
      "train Loss: 2.5246 Acc: 0.1875=3/16\n",
      "train Loss: 2.4955 Acc: 0.0625=1/16\n",
      "train Loss: 2.3915 Acc: 0.0625=1/16\n",
      "train Loss: 2.8505 Acc: 0.1250=2/16\n",
      "train Loss: 2.7728 Acc: 0.0625=1/16\n",
      "train Loss: 2.6350 Acc: 0.0625=1/16\n",
      "train Loss: 2.6386 Acc: 0.0625=1/16\n",
      "train Loss: 2.3069 Acc: 0.1250=2/16\n",
      "train Loss: 2.4140 Acc: 0.1250=2/16\n",
      "train Loss: 2.3416 Acc: 0.1250=2/16\n",
      "train Loss: 2.9069 Acc: 0.0625=1/16\n",
      "train Loss: 2.4518 Acc: 0.1250=2/16\n",
      "train Loss: 2.7231 Acc: 0.0625=1/16\n",
      "train Loss: 2.4293 Acc: 0.3125=5/16\n",
      "train Loss: 2.4594 Acc: 0.0000=0/16\n",
      "train Loss: 2.7405 Acc: 0.0625=1/16\n",
      "train Loss: 2.6135 Acc: 0.0625=1/16\n",
      "train Loss: 2.3826 Acc: 0.0625=1/16\n",
      "train Loss: 2.7045 Acc: 0.0625=1/16\n",
      "train Loss: 2.3283 Acc: 0.1250=2/16\n",
      "train Loss: 2.2828 Acc: 0.1875=3/16\n",
      "train Loss: 2.5342 Acc: 0.0625=1/16\n",
      "train Loss: 2.6408 Acc: 0.1250=2/16\n",
      "train Loss: 2.6598 Acc: 0.1250=2/16\n",
      "train Loss: 2.5312 Acc: 0.1875=3/16\n",
      "train Loss: 2.6704 Acc: 0.0625=1/16\n",
      "train Loss: 2.5550 Acc: 0.0625=1/16\n",
      "train Loss: 2.6198 Acc: 0.0625=1/16\n",
      "train Loss: 2.3983 Acc: 0.1875=3/16\n",
      "train Loss: 2.6050 Acc: 0.0000=0/16\n",
      "train Loss: 2.5499 Acc: 0.1250=2/16\n",
      "train Loss: 2.3787 Acc: 0.0625=1/16\n",
      "train Loss: 2.3873 Acc: 0.1250=2/16\n",
      "train Loss: 2.5819 Acc: 0.0000=0/16\n",
      "train Loss: 2.6276 Acc: 0.0625=1/16\n",
      "train Loss: 2.6746 Acc: 0.0625=1/16\n",
      "train Loss: 2.5347 Acc: 0.0625=1/16\n",
      "train Loss: 2.2663 Acc: 0.1875=3/16\n",
      "train Loss: 2.6684 Acc: 0.0625=1/16\n",
      "train Loss: 2.7808 Acc: 0.2500=4/16\n",
      "train Loss: 2.5555 Acc: 0.0000=0/16\n",
      "train Loss: 2.1893 Acc: 0.1875=3/16\n",
      "train Loss: 2.5262 Acc: 0.1875=3/16\n",
      "train Loss: 2.6903 Acc: 0.1250=2/16\n",
      "train Loss: 2.6025 Acc: 0.1875=3/16\n",
      "train Loss: 2.5664 Acc: 0.1250=2/16\n",
      "train Loss: 2.8282 Acc: 0.0625=1/16\n",
      "train Loss: 2.7694 Acc: 0.0000=0/16\n",
      "train Loss: 2.3950 Acc: 0.0000=0/16\n",
      "train Loss: 2.4748 Acc: 0.0625=1/16\n",
      "train Loss: 2.6350 Acc: 0.0625=1/16\n",
      "train Loss: 2.5348 Acc: 0.1250=2/16\n",
      "train Loss: 2.5452 Acc: 0.1250=2/16\n",
      "train Loss: 2.7097 Acc: 0.0000=0/16\n",
      "train Loss: 2.5074 Acc: 0.0000=0/16\n",
      "train Loss: 2.4640 Acc: 0.0625=1/16\n",
      "train Loss: 2.2706 Acc: 0.0625=1/16\n",
      "train Loss: 2.2828 Acc: 0.1875=3/16\n",
      "train Loss: 2.6600 Acc: 0.0625=1/16\n",
      "train Loss: 2.5185 Acc: 0.0625=1/16\n",
      "train Loss: 2.5606 Acc: 0.0000=0/16\n",
      "train Loss: 2.4658 Acc: 0.1250=2/16\n",
      "train Loss: 2.8085 Acc: 0.0625=1/16\n",
      "train Loss: 2.1352 Acc: 0.3750=6/16\n",
      "train Loss: 2.5370 Acc: 0.0000=0/16\n",
      "train Loss: 2.8439 Acc: 0.0000=0/16\n",
      "train Loss: 2.5610 Acc: 0.0000=0/16\n",
      "train Loss: 2.4176 Acc: 0.1875=3/16\n",
      "train Loss: 2.7011 Acc: 0.0625=1/16\n",
      "train Loss: 2.1175 Acc: 0.1250=2/16\n",
      "train Loss: 2.2753 Acc: 0.1875=3/16\n",
      "train Loss: 2.6592 Acc: 0.0625=1/16\n",
      "train Loss: 2.4229 Acc: 0.1875=3/16\n",
      "train Loss: 2.6278 Acc: 0.1250=2/16\n",
      "train Loss: 2.3702 Acc: 0.1250=2/16\n",
      "train Loss: 2.7460 Acc: 0.0625=1/16\n",
      "train Loss: 2.6156 Acc: 0.0000=0/16\n",
      "train Loss: 2.4185 Acc: 0.1875=3/16\n",
      "train Loss: 2.7528 Acc: 0.0625=1/16\n",
      "train Loss: 2.4233 Acc: 0.1250=2/16\n",
      "train Loss: 2.6715 Acc: 0.1250=2/16\n",
      "train Loss: 2.4712 Acc: 0.1250=2/16\n",
      "train Loss: 2.4050 Acc: 0.1875=3/16\n",
      "train Loss: 40.6826 Acc: 1.5664\n",
      "val Loss: 3.0300 Acc: 0.0000=0/16\n",
      "val Loss: 2.2217 Acc: 0.2500=4/16\n",
      "val Loss: 3.1880 Acc: 0.0000=0/16\n",
      "val Loss: 2.5250 Acc: 0.1875=3/16\n",
      "val Loss: 2.7141 Acc: 0.1250=2/16\n",
      "val Loss: 2.2022 Acc: 0.1875=3/16\n",
      "val Loss: 3.0964 Acc: 0.0625=1/16\n",
      "val Loss: 2.5830 Acc: 0.0000=0/16\n",
      "val Loss: 2.2282 Acc: 0.0625=1/16\n",
      "val Loss: 2.8926 Acc: 0.0000=0/16\n",
      "val Loss: 2.6601 Acc: 0.0000=0/16\n",
      "val Loss: 2.6446 Acc: 0.1250=2/16\n",
      "val Loss: 2.4750 Acc: 0.1875=3/16\n",
      "val Loss: 2.4798 Acc: 0.0000=0/16\n",
      "val Loss: 2.5892 Acc: 0.0625=1/16\n",
      "val Loss: 2.5496 Acc: 0.1250=2/16\n",
      "val Loss: 2.6432 Acc: 0.1250=2/16\n",
      "val Loss: 2.7525 Acc: 0.1250=2/16\n",
      "val Loss: 2.6505 Acc: 0.0625=1/16\n",
      "val Loss: 2.8963 Acc: 0.0000=0/16\n",
      "val Loss: 2.5655 Acc: 0.0625=1/16\n",
      "val Loss: 2.3369 Acc: 0.1875=3/16\n",
      "val Loss: 2.5569 Acc: 0.0000=0/16\n",
      "val Loss: 2.3247 Acc: 0.1875=3/16\n",
      "val Loss: 2.3244 Acc: 0.1875=3/16\n",
      "val Loss: 2.4218 Acc: 0.1875=3/16\n",
      "val Loss: 2.5657 Acc: 0.0000=0/16\n",
      "val Loss: 2.4273 Acc: 0.1875=3/16\n",
      "val Loss: 2.8741 Acc: 0.0625=1/16\n",
      "val Loss: 2.3355 Acc: 0.1875=3/16\n",
      "val Loss: 2.9911 Acc: 0.1250=2/16\n",
      "val Loss: 3.0246 Acc: 0.0000=0/16\n",
      "val Loss: 2.7965 Acc: 0.0000=0/16\n",
      "val Loss: 2.4773 Acc: 0.0625=1/16\n",
      "val Loss: 2.6203 Acc: 0.0625=1/16\n",
      "val Loss: 2.4442 Acc: 0.1875=3/16\n",
      "val Loss: 2.5490 Acc: 0.1875=3/16\n",
      "val Loss: 2.4224 Acc: 0.0625=1/16\n",
      "val Loss: 2.6118 Acc: 0.1250=2/16\n",
      "val Loss: 2.4210 Acc: 0.0000=0/16\n",
      "val Loss: 2.8109 Acc: 0.1250=2/16\n",
      "val Loss: 2.5866 Acc: 0.0000=0/16\n",
      "val Loss: 2.6150 Acc: 0.0000=0/16\n",
      "val Loss: 2.9610 Acc: 0.0625=1/16\n",
      "val Loss: 2.3881 Acc: 0.1250=2/16\n",
      "val Loss: 2.7420 Acc: 0.0625=1/16\n",
      "val Loss: 3.0404 Acc: 0.1250=2/16\n",
      "val Loss: 2.7941 Acc: 0.0625=1/16\n",
      "val Loss: 2.5496 Acc: 0.0625=1/16\n",
      "val Loss: 2.4951 Acc: 0.2500=4/16\n",
      "val Loss: 2.7412 Acc: 0.1250=2/16\n",
      "val Loss: 2.5574 Acc: 0.0625=1/16\n",
      "val Loss: 2.9051 Acc: 0.1250=2/16\n",
      "val Loss: 2.6607 Acc: 0.0000=0/16\n",
      "val Loss: 3.0267 Acc: 0.0000=0/16\n",
      "val Loss: 2.8835 Acc: 0.1250=2/16\n",
      "val Loss: 2.6415 Acc: 0.0625=1/16\n",
      "val Loss: 2.8004 Acc: 0.0000=0/16\n",
      "val Loss: 2.4487 Acc: 0.1875=3/16\n",
      "val Loss: 2.7579 Acc: 0.0625=1/16\n",
      "val Loss: 2.6763 Acc: 0.0625=1/16\n",
      "val Loss: 2.4814 Acc: 0.0625=1/16\n",
      "val Loss: 2.8829 Acc: 0.0625=1/16\n",
      "val Loss: 42.3300 Acc: 1.5664\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 2.0852 Acc: 0.1875=3/16\n",
      "train Loss: 2.5402 Acc: 0.1250=2/16\n",
      "train Loss: 2.6358 Acc: 0.1250=2/16\n",
      "train Loss: 2.7555 Acc: 0.0625=1/16\n",
      "train Loss: 2.4042 Acc: 0.1250=2/16\n",
      "train Loss: 2.4563 Acc: 0.1250=2/16\n",
      "train Loss: 2.3079 Acc: 0.1875=3/16\n",
      "train Loss: 2.5228 Acc: 0.0000=0/16\n",
      "train Loss: 2.4937 Acc: 0.1250=2/16\n",
      "train Loss: 2.3918 Acc: 0.1250=2/16\n",
      "train Loss: 2.5168 Acc: 0.0000=0/16\n",
      "train Loss: 2.4590 Acc: 0.1875=3/16\n",
      "train Loss: 2.4424 Acc: 0.0625=1/16\n",
      "train Loss: 2.8779 Acc: 0.0000=0/16\n",
      "train Loss: 2.2425 Acc: 0.1250=2/16\n",
      "train Loss: 2.5294 Acc: 0.1250=2/16\n",
      "train Loss: 2.3731 Acc: 0.0000=0/16\n",
      "train Loss: 2.6271 Acc: 0.0625=1/16\n",
      "train Loss: 2.5323 Acc: 0.1250=2/16\n",
      "train Loss: 2.6620 Acc: 0.1250=2/16\n",
      "train Loss: 2.4549 Acc: 0.1875=3/16\n",
      "train Loss: 2.5114 Acc: 0.0625=1/16\n",
      "train Loss: 2.4152 Acc: 0.0625=1/16\n",
      "train Loss: 2.2090 Acc: 0.1250=2/16\n",
      "train Loss: 2.7275 Acc: 0.0625=1/16\n",
      "train Loss: 2.7952 Acc: 0.0625=1/16\n",
      "train Loss: 2.4812 Acc: 0.0000=0/16\n",
      "train Loss: 2.4071 Acc: 0.0625=1/16\n",
      "train Loss: 2.6907 Acc: 0.1250=2/16\n",
      "train Loss: 2.2749 Acc: 0.2500=4/16\n",
      "train Loss: 2.6750 Acc: 0.0000=0/16\n",
      "train Loss: 2.5682 Acc: 0.1250=2/16\n",
      "train Loss: 2.2427 Acc: 0.1875=3/16\n",
      "train Loss: 2.6085 Acc: 0.0625=1/16\n",
      "train Loss: 3.0383 Acc: 0.0000=0/16\n",
      "train Loss: 2.4503 Acc: 0.0625=1/16\n",
      "train Loss: 2.7947 Acc: 0.1250=2/16\n",
      "train Loss: 2.4652 Acc: 0.0625=1/16\n",
      "train Loss: 2.5399 Acc: 0.2500=4/16\n",
      "train Loss: 2.5191 Acc: 0.2500=4/16\n",
      "train Loss: 2.4474 Acc: 0.0625=1/16\n",
      "train Loss: 2.6533 Acc: 0.1250=2/16\n",
      "train Loss: 2.4166 Acc: 0.1250=2/16\n",
      "train Loss: 2.5736 Acc: 0.0000=0/16\n",
      "train Loss: 2.5100 Acc: 0.0625=1/16\n",
      "train Loss: 2.6966 Acc: 0.0000=0/16\n",
      "train Loss: 2.7975 Acc: 0.0625=1/16\n",
      "train Loss: 2.4206 Acc: 0.1250=2/16\n",
      "train Loss: 2.6446 Acc: 0.0000=0/16\n",
      "train Loss: 2.4412 Acc: 0.0625=1/16\n",
      "train Loss: 2.5692 Acc: 0.2500=4/16\n",
      "train Loss: 2.3411 Acc: 0.1250=2/16\n",
      "train Loss: 2.4341 Acc: 0.1250=2/16\n",
      "train Loss: 2.7703 Acc: 0.0000=0/16\n",
      "train Loss: 2.3075 Acc: 0.0000=0/16\n",
      "train Loss: 2.4733 Acc: 0.0625=1/16\n",
      "train Loss: 2.6938 Acc: 0.0625=1/16\n",
      "train Loss: 2.4301 Acc: 0.0625=1/16\n",
      "train Loss: 2.4557 Acc: 0.1875=3/16\n",
      "train Loss: 2.4951 Acc: 0.0625=1/16\n",
      "train Loss: 2.4940 Acc: 0.0625=1/16\n",
      "train Loss: 2.4491 Acc: 0.0625=1/16\n",
      "train Loss: 2.7667 Acc: 0.0625=1/16\n",
      "train Loss: 2.3666 Acc: 0.0000=0/16\n",
      "train Loss: 2.7640 Acc: 0.0625=1/16\n",
      "train Loss: 2.3829 Acc: 0.1875=3/16\n",
      "train Loss: 2.6552 Acc: 0.0625=1/16\n",
      "train Loss: 2.5585 Acc: 0.1250=2/16\n",
      "train Loss: 2.4964 Acc: 0.0000=0/16\n",
      "train Loss: 2.2596 Acc: 0.0625=1/16\n",
      "train Loss: 2.4931 Acc: 0.0625=1/16\n",
      "train Loss: 2.3243 Acc: 0.0625=1/16\n",
      "train Loss: 2.4735 Acc: 0.1875=3/16\n",
      "train Loss: 2.5611 Acc: 0.0625=1/16\n",
      "train Loss: 2.3146 Acc: 0.0625=1/16\n",
      "train Loss: 2.3664 Acc: 0.1875=3/16\n",
      "train Loss: 2.4834 Acc: 0.0625=1/16\n",
      "train Loss: 2.4104 Acc: 0.0625=1/16\n",
      "train Loss: 2.3064 Acc: 0.1250=2/16\n",
      "train Loss: 2.4145 Acc: 0.0625=1/16\n",
      "train Loss: 2.3656 Acc: 0.1875=3/16\n",
      "train Loss: 2.4914 Acc: 0.1250=2/16\n",
      "train Loss: 2.3985 Acc: 0.1250=2/16\n",
      "train Loss: 2.5211 Acc: 0.1875=3/16\n"
     ]
    }
   ],
   "source": [
    "# 使用预训练模型\n",
    "\n",
    "cifar10RES = models.resnet18(pretrained=True)\n",
    "num_ftrs = cifar10RES.fc.in_features\n",
    "cifar10RES.fc = nn.Linear(num_ftrs, 10)\n",
    "\n",
    "num_layers = len(list(cifar10RES.parameters()))\n",
    "# 冻结模型中除了最后两层外所有层\n",
    "for iter_layer, p in enumerate(cifar10RES.parameters()):\n",
    "    if iter_layer < num_layers - 1:\n",
    "        p.require_grad = False\n",
    "    else:\n",
    "        p.require_grad = True\n",
    "        \n",
    "optimizerRES = optim.Adam(params=filter(lambda p:p.require_grad, cifar10RES.parameters()), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "exp_lr_schedulerRES = lr_scheduler.StepLR(optimizerRES, step_size=10, gamma=0.1)\n",
    "loss_fn_RES = nn.CrossEntropyLoss()\n",
    "\n",
    "train_model(cifar10RES, loss_fn_RES, optimizerRES, exp_lr_schedulerRES, dataloaders, skip=10, num_epochs=25)\n",
    "# 准确率稳定在16%左右"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
