{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch\n",
    "\n",
    "- torch.nn.Transformer\n",
    "- torch.nn.TransformerEncoder\n",
    "- torch.nn.TransformerDecoder\n",
    "- torch.nn.TransformerEncoderLayer\n",
    "- torch.nn.TransformerDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## nn.Transformer\n",
    "\n",
    "torch.nn.Transformer(d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, \n",
    "                     dropout=0.1, activation='relu', custom_encoder=None, custom_decoder=None)\n",
    "\n",
    "参数：\n",
    "- d_model 编码器，解码器默认输入大小\n",
    "- nhead 多头注意力模型的头数\n",
    "- num_encoder_layers，编码器中子编码器层的数量\n",
    "- num_decoder_layers，解码器层数\n",
    "- dim_feedforward，前馈网络模型的中间层维度\n",
    "- dropout，\n",
    "- activation，编码器/解码器中间层的激活函数，relu或gelu\n",
    "\n",
    "forward函数\n",
    "forward(src, tgt, src_mask=None, tgt_mask=None, memory_mask=None, \n",
    "        src_key_padding_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None)\n",
    "\n",
    "因为src和tgt的batch_size相同，所以判断n为batch_size，S为src的长度，t为tgt的长度。\n",
    "\n",
    "\n",
    "Shape:\n",
    "- src: (S, N, E)\n",
    "- tgt: (T, N, E)\n",
    "- src_mask: (S, S)，看得到用float('-inf')，看不到用float('0.0')\n",
    "- tgt_mask: (T, T)\n",
    "- memory_mask: (T, S)\n",
    "- src_key_padding_mask: (N, S),ByteTensor，判断这个词是否为padding\n",
    "- tgt_key_padding_mask: (N, T),ByteTensor，判断tgt中这个词是否为padding\n",
    "- memory_key_padding_mask: (N, S),ByteTensor，在tgt生成某个词时，能看到src哪些词的信息。\n",
    "    \n",
    "mask\n",
    "- generate_square_subsequent_mask(sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TransformerEncoder\n",
    "\n",
    "init:\n",
    "    torch.nn.TransformerDecoder(encoder_layer, num_layers, norm=None)\n",
    "TransformerEncoder是N个编码器层的堆叠\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "src=torch.rand(10, 32, 512)\n",
    "\n",
    "out=transformer_encoder(src)\n",
    "print(out.size()) # 10, 32, 512，跟输入一样的大小\n",
    "\n",
    "# transformer_encoder.forward(src, mask=None, src_key_padding_mask=None)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## TransformerDecoder\n",
    "init\n",
    "torch.nn.TransformerDecoder(decoder_layer, num_layers, norm=None)\n",
    "\n",
    "# transformerDecoder是N个解码器层的堆叠\n",
    "\n",
    "decoder_layer - TransformerDecoderLayer()类的实例\n",
    "num_layers，解码器中子解码器层的数量\n",
    "norm 层归一化组件\n",
    "\n",
    "decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
    "transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
    "memory = torch.rand(10, 32, 512) # encoder层的最后一层参数\n",
    "tgt = torch.rand(20, 32, 512)\n",
    "out = transformer_decoder(tgt, memory)\n",
    "print(out.size()) # torch.Size([20, 32, 512])\n",
    "\n",
    "forward(tgt, memory, tgt_mask=None, memory_mask=None, \n",
    "        tgt_key_padding_mask=None, memory_key_padding_mask=None)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### 4 TransformerEncoderLayer\n",
    "\n",
    "torch.nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu')\n",
    "\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "src = torch.rand(10, 32, 512)\n",
    "out = encoder_layer(src) # torch.Size([10, 32, 512])\n",
    "\n",
    "forward(src, src_mask=None, src_key_padding_mask=None)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### 5 TransformerDecoderLayer\n",
    "\n",
    "init\n",
    "torch.nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu')\n",
    "\n",
    "decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
    "memory = torch.rand(10, 32, 512)\n",
    "tgt = torch.rand(20, 32, 512)\n",
    "out = decoder_layer(tgt, memory) # torch.Size([20, 32, 512])\n",
    "\n",
    "# 能不能通过训练memory，使得其具有领域自适应性，语言自适应性\n",
    "\n",
    "forward(tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None)\n",
    "\n",
    "# 问题：tgt是什么"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### 6 MultiheadAttention\n",
    "\n",
    "torch.nn.MultiheadAttention(embed_dim, num_heads, dropout=0.0, bias=True, add_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
    "\n",
    "forward(query, key, value, key_padding_mask=None, need_weights=True, attn_mask=None)\n",
    "\n",
    "Shape\n",
    "query (L, N, E)\n",
    "key (S N E)\n",
    "value (S N E)\n",
    "key_padding_mask(N S)\n",
    "attn_mask (L S)\n",
    "\n",
    "outputs\n",
    "attn_output (L N E)\n",
    "attn_output_weights (N L S)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
